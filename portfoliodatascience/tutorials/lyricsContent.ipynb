{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word usage by multiple artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It obtains all lyrics for a given set of artists (parses HTML from www.metrolyrics.com) and returns a comparision of the words used by the artists.\n",
    "Packages required: requests | re | time |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "from lyricsFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching eddie-vedder songs online ...\n",
      "http://www.metrolyrics.com/eddie-vedder-lyrics.html\n",
      "Fetching dire-straits songs online ...\n",
      "http://www.metrolyrics.com/dire-straits-lyrics.html\n",
      "Fetching ed-sheeran songs online ...\n",
      "http://www.metrolyrics.com/ed-sheeran-lyrics.html\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "listOfArtists = ['Eddie Vedder', 'Dire Straits', 'Ed Sheeran']\n",
    "songs4Artists = getSongs4Artists(listOfArtists) #by default (location = 'online')\n",
    "#songs4Artists['eddie-vedder'] # Check artist songs found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each artist, collect all words across all song lyrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 20 lyrics for eddie-vedder ...\n",
      "Fetching 20 lyrics for dire-straits ...\n",
      "Fetching 20 lyrics for ed-sheeran ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "lyrics4Artists = getLyrics4Artists(songs4Artists, numSongs = 20) # by default (numSongs = 5 | location = 'online')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word extraction from artists lyrics and model generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, cv, vec, tf, labels, lyrics = tokenLyrics4Artists(lyrics4Artists, method='countVectorize')\n",
    "m = buildNaiveBayesModel(X, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of each of the artist to write the following songs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test songs might belong to:\n",
      "['dire-straits' 'eddie-vedder' 'ed-sheeran' 'dire-straits' 'ed-sheeran'\n",
      " 'eddie-vedder']\n",
      "\n",
      " Each song probability from being from each artist:\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.19243111 0.26176122 0.54580766]\n",
      " [0.34739105 0.48218501 0.17042394]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.18398068 0.5970597  0.21895962]\n",
      " [0.2689354  0.32663697 0.40442763]]\n",
      "[[-3.48106978 -3.79123309 -4.44502338 -4.07861287 -3.68494692 -3.43483466\n",
      "  -4.33204056 -4.0201482  -4.42184004 -3.77976115 -3.41250767 -3.30932341\n",
      "  -3.99825536 -4.22120126 -4.27774692 -3.64934928 -3.44799872 -3.74204074\n",
      "  -4.11573846 -3.80013857 -3.49285035 -3.62030774 -4.13159718 -4.02748674\n",
      "  -3.41040474 -3.60363609 -3.67119717 -4.13342596 -4.50152709 -3.71326611\n",
      "  -4.01363759 -4.07872185 -4.26792197 -3.90221334 -3.4190417  -4.16032034\n",
      "  -4.1878773  -4.04196922 -4.02552189 -3.6335321  -4.48072712 -4.03089658\n",
      "  -4.19065933 -3.34341734 -3.70281448 -3.47108615]\n",
      " [-4.22008075 -3.46335631 -3.69767867 -3.59319441 -4.13598774 -3.32009234\n",
      "  -3.58761164 -4.07679666 -3.95259087 -4.35271369 -4.36317661 -4.27430037\n",
      "  -3.95911117 -3.72062006 -3.99907378 -3.50702462 -3.59439302 -3.45518347\n",
      "  -4.04260642 -4.13162162 -3.75177075 -3.46372944 -3.82896552 -3.86426371\n",
      "  -2.66695563 -4.22261568 -4.03500525 -4.23169564 -3.7922561  -3.90198274\n",
      "  -3.10022668 -4.11685666 -4.07354381 -4.1179609  -4.37663138 -3.70644598\n",
      "  -4.1377701  -4.15639812 -4.32114917 -3.81140064 -3.76693524 -3.75860228\n",
      "  -4.15160488 -3.90704386 -4.54904371 -4.29187862]\n",
      " [-3.71523329 -4.50339508 -4.31937333 -4.14317485 -4.07109481 -4.15854484\n",
      "  -4.03146116 -4.20916149 -3.39772135 -3.41957341 -3.84631721 -3.65332577\n",
      "  -3.97852459 -3.89714648 -4.08533488 -4.23533922 -4.00091581 -3.21385943\n",
      "  -3.68715861 -3.77997381 -3.65356351 -2.98579536 -3.34576463 -4.45029198\n",
      "  -3.78791427 -4.36279892 -4.45746043 -3.68807156 -3.52153468 -4.3224095\n",
      "  -3.45671142 -4.39597079 -3.85992035 -4.23914788 -4.40226239 -4.17759627\n",
      "  -3.94128256 -3.8851719  -3.93127022 -3.9138828  -3.76002342 -3.20029952\n",
      "  -3.99735437 -3.56539227 -3.52639012 -4.18070419]]\n"
     ]
    }
   ],
   "source": [
    "test_songs = [\n",
    "      \"I played the blues on twelve bars down on Lover's Lane\",\n",
    "      \"I'll keep on healing all the scars That we've collected from the start\",\n",
    "      \"So, baby, now Take me into your loving arms\",\n",
    "      \"with a little help from my crocodile\",\n",
    "      \"oh sweet love, what is this\",\n",
    "      \"the beautiful people\"]\n",
    "\n",
    "prediction, classProb, logProb = proba_Lyrics4Artists(test_songs, m, cv, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which world is characteristic of each artist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordsEddieVedder = \" \".join(lyrics4Artists['eddie-vedder'])\n",
    "allWordsDireStraits = \" \".join(lyrics4Artists['dire-straits'])\n",
    "allWordsEdSheeran = \" \".join(lyrics4Artists['ed-sheeran']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(num = None, figsize = (20,20))\n",
    "\n",
    "wordcloud1 = wordcloud.WordCloud(background_color=\"white\", max_words=2000, contour_color='steelblue').generate(allWordsEddieVedder)\n",
    "plt.subplot(3,1,1)\n",
    "plt.title('eddie-vedder', fontsize=18, loc='right')\n",
    "plt.imshow(wordcloud1, interpolation='bilinear')\n",
    "\n",
    "wordcloud2 = wordcloud.WordCloud(background_color=\"white\", max_words=2000, contour_color='steelblue').generate(allWordsDireStraits)\n",
    "plt.subplot(3,1,2)\n",
    "plt.title('dire-straits', fontsize=16, loc='right')\n",
    "plt.imshow(wordcloud2, interpolation='bilinear')\n",
    "\n",
    "wordcloud3 = wordcloud.WordCloud(background_color=\"white\", max_words=2000, contour_color='steelblue').generate(allWordsEdSheeran)\n",
    "plt.subplot(3,1,3)\n",
    "plt.title('ed-sheeran', fontsize=16, loc='right')\n",
    "plt.imshow(wordcloud3, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary that better charaterize the difference between artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "away       28.623457\n",
      "just       17.386976\n",
      "hand       16.769050\n",
      "tonight    13.272100\n",
      "know        9.263679\n",
      "heart       8.266608\n",
      "cause       6.947180\n",
      "way         6.913296\n",
      "home        5.985650\n",
      "eyes        4.796726\n",
      "old         4.605908\n",
      "night       4.487066\n",
      "look        4.099975\n",
      "time        3.628698\n",
      "ve          3.424421\n",
      "love        2.641623\n",
      "oh          0.756410\n",
      "right       0.100731\n",
      "got         0.100055\n",
      "life        0.100046\n",
      "Name: 0, dtype: float64\n",
      "gonna     28.873492\n",
      "don       23.916052\n",
      "know      10.433055\n",
      "right      8.956631\n",
      "baby       8.713524\n",
      "need       6.250706\n",
      "ve         5.355179\n",
      "day        5.150983\n",
      "say        5.097462\n",
      "oh         4.099984\n",
      "yeah       4.042354\n",
      "long       3.708463\n",
      "let        3.594336\n",
      "fall       3.100024\n",
      "heart      2.333905\n",
      "make       2.323333\n",
      "got        2.104840\n",
      "just       1.992104\n",
      "people     1.679233\n",
      "like       1.511514\n",
      "Name: 1, dtype: float64\n",
      "long       48.491444\n",
      "feel       17.936929\n",
      "love       16.719125\n",
      "home       11.035059\n",
      "say         6.862082\n",
      "ve          5.414832\n",
      "need        4.281626\n",
      "like        2.982186\n",
      "day         2.477467\n",
      "just        2.309917\n",
      "baby        2.219235\n",
      "fall        2.100012\n",
      "eyes        2.100009\n",
      "tonight     1.729838\n",
      "right       1.272035\n",
      "night       1.243903\n",
      "know        1.147604\n",
      "time        1.107713\n",
      "oh          0.390347\n",
      "old         0.100014\n",
      "Name: 2, dtype: float64\n",
      "love      47.581736\n",
      "know      37.328112\n",
      "like      33.181681\n",
      "just      24.216596\n",
      "don       22.788886\n",
      "ll        18.854296\n",
      "baby      18.360929\n",
      "heart     16.828793\n",
      "said      16.086488\n",
      "right     11.856174\n",
      "place      9.028664\n",
      "way        7.372106\n",
      "oh         6.048140\n",
      "away       5.120629\n",
      "fall       4.255529\n",
      "say        3.723717\n",
      "make       3.000795\n",
      "people     2.470163\n",
      "got        2.238757\n",
      "mind       2.099990\n",
      "Name: 3, dtype: float64\n",
      "need       51.571187\n",
      "come       42.683099\n",
      "don        30.173754\n",
      "man        25.438688\n",
      "like       14.979725\n",
      "baby       14.590917\n",
      "day         7.444151\n",
      "want        6.900743\n",
      "say         4.777921\n",
      "night       4.429190\n",
      "heart       4.263702\n",
      "singing     4.249683\n",
      "make        4.178208\n",
      "cause       4.169274\n",
      "oh          4.005951\n",
      "people      2.716119\n",
      "mind        2.433601\n",
      "free        2.394725\n",
      "know        2.159217\n",
      "place       2.133537\n",
      "Name: 4, dtype: float64\n",
      "world      23.757911\n",
      "people     18.526021\n",
      "oh          8.484895\n",
      "just        4.517876\n",
      "away        1.555891\n",
      "man         1.145980\n",
      "free        1.109053\n",
      "look        1.100010\n",
      "fall        1.099903\n",
      "night       0.100896\n",
      "say         0.100028\n",
      "life        0.100027\n",
      "make        0.100023\n",
      "tonight     0.100019\n",
      "got         0.100017\n",
      "old         0.100016\n",
      "like        0.100012\n",
      "mind        0.100012\n",
      "let         0.100011\n",
      "way         0.100011\n",
      "Name: 5, dtype: float64\n",
      "love       176.332658\n",
      "oh         113.260903\n",
      "ll          20.593189\n",
      "time        16.557235\n",
      "let         12.991646\n",
      "like         9.075224\n",
      "play         7.860767\n",
      "hold         7.453587\n",
      "tonight      7.297948\n",
      "want         6.866910\n",
      "ve           6.133713\n",
      "just         6.039737\n",
      "cause        4.821890\n",
      "know         3.568104\n",
      "feel         2.301679\n",
      "mind         1.076843\n",
      "heart        0.100024\n",
      "hand         0.100022\n",
      "night        0.100022\n",
      "baby         0.100021\n",
      "Name: 6, dtype: float64\n",
      "ll       50.666124\n",
      "ve       30.171806\n",
      "got      10.752138\n",
      "let      10.489473\n",
      "life      9.467728\n",
      "home      9.024157\n",
      "feel      8.239830\n",
      "day       7.816747\n",
      "cause     7.513977\n",
      "just      4.901125\n",
      "mind      4.789594\n",
      "look      4.099963\n",
      "man       3.612243\n",
      "eyes      3.480465\n",
      "place     3.458605\n",
      "world     3.244307\n",
      "come      3.099949\n",
      "heart     2.806933\n",
      "love      2.790856\n",
      "make      2.652055\n",
      "Name: 7, dtype: float64\n",
      "way        41.917123\n",
      "free       21.697748\n",
      "time       21.277874\n",
      "yeah       21.260671\n",
      "play       20.086137\n",
      "got        17.456542\n",
      "like       15.758526\n",
      "singing    15.626290\n",
      "just       13.636418\n",
      "life       11.480756\n",
      "gonna      10.093376\n",
      "fall        8.944462\n",
      "say         8.038714\n",
      "want        7.532274\n",
      "come        7.115895\n",
      "look        7.099987\n",
      "know        6.433620\n",
      "make        6.385170\n",
      "mind        6.099907\n",
      "old         5.966652\n",
      "Name: 8, dtype: float64\n",
      "don      18.521154\n",
      "know     16.248825\n",
      "night    13.268389\n",
      "got      10.947601\n",
      "hold      9.499000\n",
      "home      9.355103\n",
      "eyes      8.556116\n",
      "make      7.060389\n",
      "time      6.928398\n",
      "yeah      6.723515\n",
      "come      6.501009\n",
      "ll        6.242956\n",
      "feel      5.921522\n",
      "like      5.798431\n",
      "just      4.916730\n",
      "love      4.458271\n",
      "right     4.124431\n",
      "man       4.094477\n",
      "let       3.099984\n",
      "baby      2.315410\n",
      "Name: 9, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#### Latent Dirichlet Allocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "wordVectors4Artists = list(sorted(cv.vocabulary_.keys()))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10)\n",
    "lda.fit(vec)\n",
    "c = lda.components_\n",
    "\n",
    "ctrans = c.T\n",
    "\n",
    "df = pd.DataFrame(ctrans, index=wordVectors4Artists)\n",
    "\n",
    "for i in range(10):\n",
    "    print(df[i].sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0\n",
      "variance_ratio PC-1  0.392267\n",
      "variance_ratio PC-2  0.311976\n",
      "variance_ratio PC-3  0.295756\n",
      "\n",
      " Principal component 1 holds 0.3922672152301272 of the variation \n",
      "\n",
      "ll         0.937231\n",
      "oh         0.871612\n",
      "love       0.836068\n",
      "know       0.787409\n",
      "ve         0.633399\n",
      "long       0.490924\n",
      "need       0.428399\n",
      "feel       0.406060\n",
      "tonight    0.264659\n",
      "like       0.235196\n",
      "way        0.181211\n",
      "eyes       0.161904\n",
      "heart      0.161559\n",
      "don        0.134586\n",
      "let        0.094435\n",
      "just       0.073936\n",
      "free       0.050328\n",
      "come       0.044471\n",
      "time       0.018920\n",
      "home      -0.019395\n",
      "Name: 0, dtype: float64\n",
      "\n",
      " Principal component 2 holds 0.3119763314169067 of the variation \n",
      "\n",
      "love     0.981261\n",
      "baby     0.721516\n",
      "don      0.714739\n",
      "home     0.564103\n",
      "just     0.410880\n",
      "come     0.326905\n",
      "look     0.326494\n",
      "night    0.305940\n",
      "man      0.300805\n",
      "oh       0.292593\n",
      "cause    0.256917\n",
      "right    0.241394\n",
      "eyes     0.193917\n",
      "time     0.145803\n",
      "make     0.121843\n",
      "old      0.091453\n",
      "play     0.081566\n",
      "like     0.069646\n",
      "heart    0.043172\n",
      "place    0.043000\n",
      "Name: 1, dtype: float64\n",
      "\n",
      " Principal component 3 holds 0.295756453352966 of the variation \n",
      "\n",
      "cause      0.674106\n",
      "tonight    0.549381\n",
      "eyes       0.503385\n",
      "need       0.502072\n",
      "hold       0.412649\n",
      "feel       0.372046\n",
      "right      0.363187\n",
      "heart      0.344347\n",
      "people     0.333235\n",
      "come       0.290640\n",
      "old        0.290585\n",
      "look       0.288963\n",
      "want       0.286169\n",
      "said       0.270286\n",
      "fall       0.189817\n",
      "let        0.145677\n",
      "mind       0.138299\n",
      "baby       0.129602\n",
      "hand       0.122388\n",
      "say        0.113543\n",
      "Name: 2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Principal components \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "wordVectors4Artists = list(sorted(cv.vocabulary_.keys()))\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "logProb_T = np.transpose(logProb)\n",
    "c = pca.fit_transform(logProb_T)\n",
    "df = pd.DataFrame(c, index=wordVectors4Artists)\n",
    "#print(df)\n",
    "varianceDf = pd.DataFrame(pca.explained_variance_ratio_, index = ['variance_ratio PC-1','variance_ratio PC-2','variance_ratio PC-3'])\n",
    "print(varianceDf)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"\\n Principal component \" + str(i + 1) + \" holds \"+ str(varianceDf.iloc[i,0]) + \" of the variation \\n\")\n",
    "    print(df[i].sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain world similarity per artist\n",
    "### Deep Learning using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gensim models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between: \n",
      "love and right 0.03530192\n",
      "love and wrong 0.2852222\n",
      "\n",
      "Difference between right and wrong: -24.99%\n"
     ]
    }
   ],
   "source": [
    "analysis_gensimModels(lyrics4Artists['eddie-vedder'], 'love', 'right', 'wrong')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between: \n",
      "love and right 0.69960564\n",
      "love and wrong 0.22024722\n",
      "\n",
      "Difference between right and wrong: 47.94%\n"
     ]
    }
   ],
   "source": [
    "analysis_gensimModels(lyrics4Artists['dire-straits'], 'love', 'right', 'wrong')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between: \n",
      "love and right 0.99618876\n",
      "love and wrong 0.9768212\n",
      "\n",
      "Difference between right and wrong: 1.94%\n"
     ]
    }
   ],
   "source": [
    "analysis_gensimModels(lyrics4Artists['ed-sheeran'], 'love', 'right', 'wrong')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained GenSim Vectors\n",
    "###### This is a 4 GB file that will be loaded to memory. It will require a lot of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.utils import get_file\n",
    "import gensim\n",
    "import subprocess\n",
    "\n",
    "MODEL = 'GoogleNews-vectors-negative300.bin'\n",
    "path = get_file(MODEL + '.gz', 'https://s3.amazonaws.com/dl4j-distribution/%s.gz' % MODEL)\n",
    "if not os.path.isdir('generated'):\n",
    "    os.mkdir('generated')\n",
    "\n",
    "unzipped = os.path.join('generated', MODEL)\n",
    "if not os.path.isfile(unzipped):\n",
    "    with open(unzipped, 'wb') as fout:\n",
    "        zcat = subprocess.Popen(['zcat'],\n",
    "                          stdin=open(path),\n",
    "                          stdout=fout\n",
    "                         )\n",
    "        zcat.wait()\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(unzipped, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "a = sys.path.insert(0, '../../../rawData')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This script loads pre-trained word embeddings (GloVe embeddings)\n",
    "into a frozen Keras Embedding layer, and uses it to\n",
    "train a text classification model on the 20 Newsgroup dataset\n",
    "(classification of newsgroup messages into 20 different categories).\n",
    "\n",
    "GloVe embedding data can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
    "\n",
    "CODE FROM:\n",
    "https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "BASE_DIR = '/Users/Magalangelo/Dropbox/_dataScienceRepos/portfolioDataScience/rawData'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, f'glove.6B.{EMBEDDING_DIM}d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "names = [x[1] for x in sorted([(b, a) for a, b in labels_index.items()])]\n",
    "\n",
    "def pred(s):\n",
    "    s1 = tokenizer.texts_to_sequences([s])\n",
    "    d1 = pad_sequences(s1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    lab = model.predict(d1)\n",
    "    for nam, score in zip(names, lab[0]):\n",
    "        if score > 0.1:\n",
    "            print(f\"{nam:25}\\t{score:6.2f}\")\n",
    "\n",
    "\n",
    "pred(\"god is a spaghetti monster floating in space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
